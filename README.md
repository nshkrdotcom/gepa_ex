# GEPA for Elixir

An Elixir implementation of GEPA (Genetic-Pareto), a framework for optimizing text-based system components using LLM-based reflection and Pareto-efficient evolutionary search.

**Status:** üéâ Phase 2 Complete - Core Features! | v0.4.0-dev

[![Tests](https://img.shields.io/badge/tests-218%2F218%20passing-brightgreen)]()
[![Coverage](https://img.shields.io/badge/coverage-75.4%25-brightgreen)]()
[![Phase 1](https://img.shields.io/badge/Phase%201-Complete-success)]()
[![Phase 2](https://img.shields.io/badge/Phase%202-Complete-success)]()

## About GEPA

GEPA optimizes arbitrary systems composed of text components‚Äîlike AI prompts, code snippets, or textual specs‚Äîagainst any evaluation metric. It employs LLMs to reflect on system behavior, using feedback from execution traces to drive targeted improvements.

This is an Elixir port of the [Python GEPA library](https://github.com/gepa-ai/gepa), designed to leverage:
- üöÄ **BEAM concurrency** for 5-10x evaluation speedup (coming in Phase 4)
- üõ°Ô∏è **OTP supervision** for fault-tolerant external service integration
- üîÑ **Functional programming** for clean, testable code
- üìä **Telemetry** for comprehensive observability (coming in Phase 3)
- ‚ú® **Production LLMs** - OpenAI GPT-4o-mini & Google Gemini 2.0 Flash

## üéâ Phase 1 Complete - Production Ready!

### Core Features (MVP - v0.1.0)

**Optimization System:**
- ‚úÖ `GEPA.optimize/1` - Public API (working!)
- ‚úÖ `GEPA.Engine` - Full optimization loop with stop conditions
- ‚úÖ `GEPA.Proposer.Reflective` - Mutation strategy
- ‚úÖ `GEPA.State` - State management with automatic Pareto updates (96.5% coverage)
- ‚úÖ `GEPA.Utils.Pareto` - Multi-objective optimization (93.5% coverage, property-verified)
- ‚úÖ `GEPA.Result` - Result analysis (100% coverage)
- ‚úÖ `GEPA.Adapters.Basic` - Q&A adapter (92.1% coverage)
- ‚úÖ Stop conditions with budget control
- ‚úÖ State persistence (save/load)
- ‚úÖ End-to-end integration tested

### Phase 1 Additions (v0.2.0) - NEW! üéâ

**Production LLM Integration:**
- ‚úÖ `GEPA.LLM` - Unified LLM behavior
- ‚úÖ `GEPA.LLM.ReqLLM` - Production implementation via ReqLLM
  - OpenAI support (GPT-4o-mini default)
  - Google Gemini support (Gemini-2.0-Flash-Exp default)
  - Error handling, retries, timeouts
  - Configurable via environment or runtime
- ‚úÖ `GEPA.LLM.Mock` - Testing implementation with flexible responses

**Advanced Batch Sampling:**
- ‚úÖ `GEPA.Strategies.BatchSampler.EpochShuffled` - Epoch-based training with shuffling
- ‚úÖ Reproducible with seed control
- ‚úÖ Better training dynamics than simple sampling

**Working Examples:**
- ‚úÖ 4 .exs script examples (quick start, math, custom adapter, persistence)
- ‚úÖ 3 Livebook notebooks (interactive learning)
- ‚úÖ Comprehensive examples/README.md guide
- ‚úÖ Livebook guide with visualizations

**Phase 2 Additions (v0.4.0) - NEW! üéâ**

**Merge Proposer:**
- ‚úÖ `GEPA.Proposer.Merge` - Genealogy-based candidate merging
- ‚úÖ `GEPA.Utils` - Pareto dominator detection (93.3% coverage)
- ‚úÖ `GEPA.Proposer.MergeUtils` - Ancestry tracking (92.3% coverage)
- ‚úÖ Engine integration with merge scheduling
- ‚úÖ 44 comprehensive tests (34 unit + 10 properties)

**Incremental Evaluation:**
- ‚úÖ `GEPA.Strategies.EvaluationPolicy.Incremental` - Progressive validation
- ‚úÖ Configurable sample sizes and thresholds
- ‚úÖ Reduces computation on large validation sets
- ‚úÖ 12 tests

**Advanced Stop Conditions:**
- ‚úÖ `GEPA.StopCondition.Timeout` - Time-based stopping
- ‚úÖ `GEPA.StopCondition.NoImprovement` - Early stopping
- ‚úÖ Flexible time units and patience settings
- ‚úÖ 9 tests

**Test Quality:**
- 201 tests (185 unit + 16 properties + 1 doctest)
- 100% passing ‚úÖ
- 75.4% coverage (excellent!)
- Property tests with 1,600+ runs
- Zero Dialyzer errors
- TDD methodology throughout

### What's Next? See the [Roadmap](docs/20251029/roadmap.md)

**‚úÖ Phase 1: Production Viability (v0.2.0)** - COMPLETE!
- ‚úÖ Real LLM integration (OpenAI, Gemini)
- ‚úÖ Quick start examples (4 scripts + 3 livebooks)
- ‚úÖ EpochShuffledBatchSampler

**‚úÖ Phase 2: Core Completeness (v0.4.0)** - COMPLETE!
- ‚úÖ Merge proposer (genealogy-based recombination)
- ‚úÖ IncrementalEvaluationPolicy (progressive validation)
- ‚úÖ Additional stop conditions (Timeout, NoImprovement)
- ‚úÖ Engine integration for merge proposer

**Phase 3: Production Hardening (v0.5.0)** - 8-10 weeks
- üì° Telemetry integration
- üé® Progress tracking
- üõ°Ô∏è Robust error handling

**Phase 4: Ecosystem Expansion (v1.0.0)** - 12-14 weeks
- üîå Additional adapters (Generic, RAG)
- üöÄ Performance optimization (parallel evaluation)
- üåü Community infrastructure

See [Implementation Gap Analysis](docs/20251029/implementation_gap_analysis.md) for detailed comparison with Python GEPA.

## Quick Start

### With Mock LLM (No API Key Required)

```elixir
# Define training data
trainset = [
  %{input: "What is 2+2?", answer: "4"},
  %{input: "What is 3+3?", answer: "6"}
]

valset = [%{input: "What is 5+5?", answer: "10"}]

# Create adapter with mock LLM (for testing)
adapter = GEPA.Adapters.Basic.new(llm: GEPA.LLM.Mock.new())

# Run optimization
{:ok, result} = GEPA.optimize(
  seed_candidate: %{"instruction" => "You are a helpful assistant."},
  trainset: trainset,
  valset: valset,
  adapter: adapter,
  max_metric_calls: 50
)

# Access results
best_program = GEPA.Result.best_candidate(result)
best_score = GEPA.Result.best_score(result)

IO.puts("Best score: #{best_score}")
IO.puts("Iterations: #{result.i}")
```

### With Production LLMs (NEW in v0.2.0!)

```elixir
# OpenAI (GPT-4o-mini) - Requires OPENAI_API_KEY
llm = GEPA.LLM.ReqLLM.new(provider: :openai)
adapter = GEPA.Adapters.Basic.new(llm: llm)

# Or Gemini (Gemini-2.0-Flash-Exp) - Requires GEMINI_API_KEY
llm = GEPA.LLM.ReqLLM.new(provider: :gemini)
adapter = GEPA.Adapters.Basic.new(llm: llm)

# Then run optimization as above
{:ok, result} = GEPA.optimize(
  seed_candidate: %{"instruction" => "..."},
  trainset: trainset,
  valset: valset,
  adapter: adapter,
  max_metric_calls: 50
)
```

See [examples/](examples/) for complete working examples!

### Interactive Livebooks (NEW!)

For interactive learning and experimentation:

```bash
# Install Livebook
mix escript.install hex livebook

# Open a livebook
livebook server livebooks/01_quick_start.livemd
```

Available Livebooks:
- `01_quick_start.livemd` - Interactive introduction
- `02_advanced_optimization.livemd` - Parameter tuning and visualization
- `03_custom_adapter.livemd` - Build adapters interactively

See [livebooks/README.md](livebooks/README.md) for details!

### With State Persistence

```elixir
{:ok, result} = GEPA.optimize(
  seed_candidate: seed,
  trainset: trainset,
  valset: valset,
  adapter: GEPA.Adapters.Basic.new(),
  max_metric_calls: 100,
  run_dir: "./my_optimization"  # State saved here, can resume
)
```

## Development

```bash
# Get dependencies
mix deps.get

# Run tests
mix test

# Run with coverage
mix test --cover

# Run specific tests
mix test test/gepa/utils/pareto_test.exs

# Format code
mix format

# Type checking
mix dialyzer
```

## Architecture

Based on behavior-driven design with functional core:

```
GEPA.optimize/1
  ‚Üì
GEPA.Engine ‚Üê Behaviors ‚Üí User Implementations
  ‚îú‚îÄ‚Üí Adapter (evaluate, reflect, propose)
  ‚îú‚îÄ‚Üí Proposer (reflective, merge)
  ‚îú‚îÄ‚Üí Strategies (selection, sampling, evaluation)
  ‚îî‚îÄ‚Üí StopCondition (budget, time, threshold)
```

## Documentation

### Current Status & Planning
- [Implementation Gap Analysis](docs/20251029/implementation_gap_analysis.md) - Detailed comparison with Python GEPA (~60% complete)
- [Development Roadmap](docs/20251029/roadmap.md) - Path to v1.0.0 (12-14 weeks)

### Technical Documentation
- [Complete Integration Guide](docs/20250829/00_complete_integration_guide.md)
- [Technical Design](docs/TECHNICAL_DESIGN.md)
- [Implementation Status](docs/IMPLEMENTATION_STATUS.md)
- [LLM Adapter Design](docs/llm_adapter_design.md) - Design for real LLM integration
- Component analysis (6 detailed documents in `docs/20250829/`)

## Related Projects

- [GEPA Python](https://github.com/gepa-ai/gepa) - Original implementation
- [GEPA Paper](https://arxiv.org/abs/2507.19457) - Research paper

## License

MIT License

